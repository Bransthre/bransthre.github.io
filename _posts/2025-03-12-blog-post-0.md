---
title: '[COMPSCI 280] Homework 2: UNet Electric Boogaloo (and Flow Matching)'
date: 2025-03-12
permalink: /posts/cs280-hw2/
tags:
  - COMPSCI 280
---

# COMPSCI 280 Homework 2 Writeup
I can't believe taking COMPSCI 180 was a season ago.

```
from torch import nn
import torch
```

## Part 1: Training a Signle-Step Denoising UNet
In this part, we train a denoiser UNet.
### 1.1 Implementing the UNet
The UNet implementation follows a very complex architecture.
At a higher level, however, we can efficiently separate the corresponding blocks into substructures.
Along the [insturctions](https://cal-cs180.github.io/fa24/hw/proj5/partb_fm.html), we can implement each marked high-level block of the UNet as follows:

#### Generalizable Block Structures
A general convolution-based block:
```
class GenericConvApplication(nn.Module):
    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        kernel_size: int,
        stride: int,
        padding: int,
        conv_type: nn.Module,
    ):
        super().__init__()
        self.conv_layer = conv_type(
            in_channels=in_channels,
            out_channels=out_channels,
            kernel_size=kernel_size,
            stride=stride,
            padding=padding,
        )
        self.batch_norm = nn.BatchNorm2d(num_features=out_channels)
        # Based on internet, torch MNIST is grayscale and 28x28
        self.gelu = nn.GELU()
        self.layers = [self.conv_layer, self.batch_norm, self.gelu]

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        for layer in self.layers:
            x = layer(x)
        return x
```
A general network block:
```
class GeneralBlock(nn.Module):
    def __init__(self, in_channels, out_channels, block_type_first, block_type_second):
        super().__init__()
        self.first_block = block_type_first(in_channels, out_channels)
        self.second_block = block_type_second(out_channels, out_channels)
        self.blocks = [self.first_block, self.second_block]

    def forward(self, x: torch.Tensor):
        for layer in self.blocks:
            x = layer(x)
        return x
```

#### High-Level Structures
A Flatten operation:
```
class Flatten(nn.Module):
    def __init__(self):
        super().__init__()
        self.layers = [nn.AvgPool2d(kernel_size=7), nn.GELU()]

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        for layer in self.layers:
            x = layer(x)
        return x
```
An Unflatten operation:
```
class Unflatten(GenericConvApplication):
    def __init__(self, in_channels: int, out_channels: int):
        super().__init__(
            in_channels=in_channels,
            out_channels=out_channels,
            kernel_size=7,
            stride=7,
            padding=0,
            conv_type=nn.ConvTranspose2d,
        )
```
A Conv operation:
```
class Conv(GenericConvApplication):
    def __init__(self, in_channels: int, out_channels: int):
        super().__init__(
            in_channels=in_channels,
            out_channels=out_channels,
            kernel_size=3,
            stride=1,
            padding=1,
            conv_type=nn.Conv2d,
        )
```
A DownConv operation:
```
class DownConv(GenericConvApplication):
    def __init__(self, in_channels: int, out_channels: int):
        super().__init__(
            in_channels=in_channels,
            out_channels=out_channels,
            kernel_size=3,
            stride=2,
            padding=1,
            conv_type=nn.Conv2d,
        )
```
An UpConv operation:
```
class UpConv(GenericConvApplication):
    def __init__(self, in_channels: int, out_channels: int):
        super().__init__(
            in_channels=in_channels,
            out_channels=out_channels,
            kernel_size=4,
            stride=2,
            padding=1,
            conv_type=nn.ConvTranspose2d,
        )
```
A ConvBlock:
```
class ConvBlock(GeneralBlock):
    def __init__(self, in_channels: int, out_channels: int):
        super().__init__(
            in_channels=in_channels,
            out_channels=out_channels,
            block_type_first=Conv,
            block_type_second=Conv,
        )
```
A DownBlock:
```
class DownBlock(GeneralBlock):
    def __init__(self, in_channels: int, out_channels: int):
        super().__init__(
            in_channels=in_channels,
            out_channels=out_channels,
            block_type_first=DownConv,
            block_type_second=ConvBlock,
        )
```
An UpBlock:
```
class UpBlock(GeneralBlock):
    def __init__(self, in_channels: int, out_channels: int):
        super().__init__(
            in_channels=in_channels,
            out_channels=out_channels,
            block_type_first=UpConv,
            block_type_second=ConvBlock,
        )
```
Finally, an FC (Fully Connected) Block:
```
class FCBlock(nn.Module):
    def __init__(self, in_channels: int, out_channels: int):
        super().__init__()
        self.first_fc_layer = nn.Linear(
            in_features=in_channels, out_features=out_channels
        )
        self.gelu_act = nn.GELU()
        self.second_fc_layer = nn.Linear(
            in_features=out_channels, out_features=out_channels
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = self.first_fc_layer(x)
        x = self.gelu_act(x)
        x = self.second_fc_layer(x)
        return x
```
#### Combining the Blocks
To make a UNet, combine the above blocks in the following manner:
```
class UnconditionalUNet(nn.Module):
    def __init__(
        self,
        in_channels: int,
        num_hiddens: int,
    ):
        super().__init__()
        self.start_conv_block = ConvBlock(
            in_channels=in_channels, out_channels=num_hiddens
        )
        self.block_a1 = DownBlock(in_channels=num_hiddens, out_channels=num_hiddens)
        self.block_a2 = DownBlock(in_channels=num_hiddens, out_channels=2 * num_hiddens)
        self.block_a3 = Flatten()

        self.block_b3 = Unflatten(
            in_channels=2 * num_hiddens, out_channels=2 * num_hiddens
        )
        self.block_b2 = UpBlock(in_channels=4 * num_hiddens, out_channels=num_hiddens)
        self.block_b1 = UpBlock(in_channels=2 * num_hiddens, out_channels=num_hiddens)
        self.end_conv_block = ConvBlock(
            in_channels=2 * num_hiddens, out_channels=num_hiddens
        )
        self.end_conv_layer = nn.Conv2d(
            in_channels=num_hiddens,
            out_channels=in_channels,
            kernel_size=3,
            stride=1,
            padding=1,
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        assert x.shape[-2:] == (28, 28), "Expect input shape to be (28, 28)."
        a0_output = self.start_conv_block(x)
        a1_output = self.block_a1(a0_output)
        a2_output = self.block_a2(a1_output)
        a3_output = self.block_a3(a2_output)

        b3_output = self.block_b3(a3_output)
        b3_cat = torch.cat((a2_output, b3_output), dim=1)  # zeroth dim is B

        b2_output = self.block_b2(b3_cat)
        b2_cat = torch.cat((a1_output, b2_output), dim=1)

        b1_output = self.block_b1(b2_cat)
        b0_output = torch.cat((a0_output, b1_output), dim=1)
        end_conv_block_output = self.end_conv_block(b0_output)
        end_conv_output = self.end_conv_layer(end_conv_block_output)
        return end_conv_output
```
### 1.2 Using the UNet to Train a Denoiser
To train a denoiser, we will use the following loss objective:

$$\mathcal{L}(\theta)=\mathbb{E}_{z,x} \|D_\theta (z) - x \|^2$$

where the loss is essentially a mean-squared-error between an original clean image, $x$, and a denoised output (where the denoiser is $D_\theta$) of a noised image $z$. Here, we noise the image manually: $z = x+\sigma \epsilon$ where $\sigma$ is the noise level and $\epsilon \sim \mathcal{N}(0, I)$ is a noise.

The noising process appears as follows:

![image](/post_assets/280_hw2_result/unet/result_B1_noising_process.png)

### 1.2.1 Training
Let's begin with some preamble:
```
import os

device = torch.cuda.current_device()
dataset = MNIST(root=".", train=True, download=True, transform=to_tensor())
test_set = MNIST(root=".", train=False, download=True, transform=to_tensor())
cwd_dir = os.path.dirname(os.path.realpath(__file__))
```

To follow the instructions of the assignment instruction (at least partially), here is how it would appear.

**Evaluator**:
```
def get_testing_results(model, noise_level):
    model = model.to(device)
    test_loader = DataLoader(test_set, batch_size=10, shuffle=True)
    for first_test_batch_img, _ in test_loader:
        first_test_batch_noised = first_test_batch_img + noise_level * torch.normal(
            0, 1, size=first_test_batch_img.shape
        )
        first_test_batch_img = first_test_batch_img.to(device)
        first_test_batch_noised = first_test_batch_noised.to(device)
        first_test_batch_denoised = model(first_test_batch_noised)
        break
    return first_test_batch_noised, first_test_batch_denoised
```
**Trainer**:
```
def unet_trainer(in_channels=1, num_hiddens=128, noise_level=0.5):
    model = UnconditionalUNet(in_channels=in_channels, num_hiddens=num_hiddens)
    model = model.to(device)

    train_loader = DataLoader(dataset, batch_size=256, shuffle=True)
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)
    n_epoch = 5

    training_losses = []
    testing_results = []
    model.eval()
    testing_results.append(get_testing_results(model, noise_level))

    for _ in range(n_epoch):
        model.train()
        running_loss = 0
        for img, lbl in tqdm.tqdm(train_loader):
            # img = to_tensor(img)
            noised_img = img + noise_level * torch.normal(0, 1, size=img.shape)
            img, noised_img = img.to(device), noised_img.to(device)

            denoised_img = model(noised_img)
            batch_loss = torch.nn.functional.mse_loss(img, denoised_img)
            training_losses.append(batch_loss)

            optimizer.zero_grad()
            batch_loss.backward()
            optimizer.step()

            running_loss += batch_loss
        model.eval()
        testing_results.append(get_testing_results(model, noise_level))

    return {
        "epoch_trianing_loss": running_loss / len(train_loader),
        "model": model,
        "training_losses": training_losses,
        "testing_results": testing_results,
    }
```
Here is how the training loss curve seems:

![image](/post_assets/280_hw2_result/unet/result_A_training_loss.png)

And here is how the overall denoising process looks like with a trained denoiser across the epochs trained:

```
fig, ax = plt.subplots(12, 10, figsize=(90, 120))
    plottable = lambda x: x.detach().cpu().permute(1, 2, 0)

    for i, epoch_data in enumerate(unet_training_outcome["testing_results"]):
        # Set row-wise titles
        for row in range(12):
            if row == 2 * i:
                ax[row, 5].set_title(f"Epoch {i} - Noised Images", fontsize=80)
            elif row == 2 * i + 1:
                ax[row, 5].set_title(f"Epoch {i} - Denoised Images", fontsize=80)

        for j_n in range(epoch_data[0].shape[0]):
            noised_img = plottable(epoch_data[0][j_n])
            denoised_img = plottable(epoch_data[1][j_n])
            ax[2 * i][j_n].imshow(noised_img, cmap="gray")
            ax[2 * i + 1][j_n].imshow(denoised_img, cmap="gray")
```

![image](/post_assets/280_hw2_result/unet/result_C_denoising_process.png)

### 1.2.2 Out-of-Distribution Testing
This is performed as detailed below:
```
test_loader = DataLoader(test_set, batch_size=5, shuffle=True)
noised_versions = {}
for first_test_batch_img, _ in test_loader:
    noise = torch.normal(0, 1, size=first_test_batch_img.shape)
    for noise_level in [0, 0.2, 0.4, 0.5, 0.6, 0.8, 1.0]:
        noised_versions[noise_level] = first_test_batch_img + noise_level * noise
    break

denoised_versions = {
    k: unet_training_outcome["model"](v.to(device))
    for k, v in noised_versions.items()
}

fig, ax = plt.subplots(5, 7, figsize=(25, 15))
plt.suptitle(r"Denoised Outputs for A UNet Trained on $\sigma=0.5$", fontsize=40)
plottable = lambda x: x.detach().cpu().permute(1, 2, 0)

for i, (noise_level, denoised_imgs) in enumerate(denoised_versions.items()):
    ax[0, i].set_title(f"Noise level: {noise_level}", fontsize=22)
    for j in range(denoised_imgs.shape[0]):
        ax[j, i].imshow(plottable(denoised_imgs[j]), cmap="gray")
```
From which you may then obtain the plot as follows that details out-of-distribution denoising on a noise level that was not trained on:


![image](/post_assets/280_hw2_result/unet/result_B2_denoising_process.png)

### 1.2.3 Denoising Pure Noise
The average training set image is seen as follows:

![image](/post_assets/280_hw2_result/unet/average_image.png)

Whereas denoising a pure Gaussian noise yields the following output at 1 epoch:

![image](/post_assets/280_hw2_result/unet/result_D_denoising_noises_epoch_1.png)

and at 5 epochs:

![image](/post_assets/280_hw2_result/unet/result_D_denoising_noises.png)

The reason why they are so distinct is because a purely Gaussian noise is very spikey across the image, and the denoiser is trained to remove such positions of spike on a similar-to-average image. In other words, it is tuned towards processing such spiky nosies as an effort to recover the existing low-frequency details on the picture. This is why the denoiser, on pure noise (or perhaps blankness), will not yield pictures with similar low-frequency details as the average image, but instead show how they would apply additional perturbations to un-noise and un-perturb an original noised copy.

## Part 2: Training a Flow Matching Model
We train a Flow Matching model, for which the details are detailed in the lecture.

Concretely, we would like to construct a denoiser $u(x_t, t)$ such that for any pair of noisy and clean image $(x_0, x_1)$ (respectively) and an arbitrary $t$ that scales the noise, it will be able to recover $x_1$ (clean image) from $x_t$ (image with noise-level $t$).

In mathematical formulation, that is to say:

$$x_t = (1-t) x_0 + t x_1, x_0 \sim \mathcal{N}(0, 1), t \in [0, 1]$$
$$u(x_t, t) = \frac{d}{dt} x_t = x_1 - x_0$$

The objective of a denoiser is to recover the image. Therefore, it is trained on the following objective as one can intuitively devise:

$$\mathcal{L}_\theta = \mathbb{E}_{x_o \sim p_0(x_0), x_1 \sim p_1(x_1), t \sim U[0, 1]} \|(x_1 - x_0) - u_\theta(x_t, t)\|^2$$

### Part 2.1 Adding Time-Conditioning to UNet
This is a simple edit of the above UNet implementation:
```
class TimeConditionalUNet(UnconditionalUNet):
    def __init__(
        self,
        in_channels: int,
        num_classes: int,
        num_hiddens: int,
    ):
        super().__init__(in_channels=in_channels, num_hiddens=num_hiddens)
        self.fc_b3 = FCBlock(1, 2 * num_hiddens)
        self.fc_b2 = FCBlock(1, num_hiddens)

    def forward(
        self,
        x: torch.Tensor,
        t: torch.Tensor,
    ) -> torch.Tensor:
        """
        Args:
            x: (N, C, H, W) input tensor.
            t: (N,) normalized time tensor.

        Returns:
            (N, C, H, W) output tensor.
        """
        assert x.shape[-2:] == (28, 28), "Expect input shape to be (28, 28)."
        a0_output = self.start_conv_block(x)
        a1_output = self.block_a1(a0_output)
        a2_output = self.block_a2(a1_output)
        a3_output = self.block_a3(a2_output)

        b3_output = (
            self.block_b3(a3_output) + self.fc_b3(t)[..., None, None]
        )  # change here
        b3_cat = torch.cat((a2_output, b3_output), dim=1)

        b2_output = (
            self.block_b2(b3_cat) + self.fc_b2(t)[..., None, None]
        )  # change here
        b2_cat = torch.cat((a1_output, b2_output), dim=1)

        b1_output = self.block_b1(b2_cat)
        b0_output = torch.cat((a0_output, b1_output), dim=1)
        end_conv_block_output = self.end_conv_block(b0_output)
        end_conv_output = self.end_conv_layer(end_conv_block_output)
        return end_conv_output
```

### Part 2.2 Training the UNet
We first construct the following utilizations to implement the loss computation and sampling procedure for our time-conditioned UNet.
```
def fm_forward_time_cond(
    unet: TimeConditionalUNet,
    x_0: torch.Tensor,
    x_1: torch.Tensor,
    x_t: torch.Tensor,
    ts: torch.Tensor,
) -> torch.Tensor:
    unet.train()
    true_velocity = x_1 - x_0
    true_velocity = true_velocity.to(device)
    pred_velocity = unet(x_t.to(device), ts[..., None].to(device))

    batch_loss = torch.nn.functional.mse_loss(true_velocity, pred_velocity)

    return batch_loss.mean()


@torch.inference_mode()
def fm_sample_time(
    unet: TimeConditionalUNet,
    img_wh: tuple[int, int],
    num_ts: int,
) -> torch.Tensor:
    unet.eval()
    timesteps = torch.linspace(0, 1, num_ts).to(device)[..., None]
    curr_step_img = torch.randn((1, 1, *img_wh)).to(device)
    for curr_t in timesteps:
        curr_step_img += unet(curr_step_img, curr_t) / num_ts
    return curr_step_img
```
The trainer is as formulated below:
```
def fm_time_cond_train():
    # Define based on assignment:
    num_hiddens = 64
    in_channels = 1

    # Train with fm_forward, borrow code from above
    model = TimeConditionalUNet(
        in_channels=in_channels, num_classes=10, num_hiddens=num_hiddens
    )
    model = model.to(device)

    train_loader = DataLoader(dataset, batch_size=64, shuffle=True)
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)
    n_epoch = 20
    lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(
        optimizer, gamma=0.1 ** (1 / n_epoch)
    )

    training_losses_time = []
    testing_results = []
    model.eval()
    testing_results.append([fm_sample_time(model, (28, 28), 299) for _ in range(5)])
    for _ in range(n_epoch):
        model.train()
        running_loss = 0
        for img, lbl in tqdm.tqdm(train_loader):
            rand_t = torch.rand(size=(img.shape[0],))
            x0 = torch.randn(img.shape)
            rand_t_reshaped = rand_t.reshape((rand_t.shape[0], 1, 1, 1))
            xt = (1 - rand_t_reshaped) * x0 + rand_t_reshaped * img
            img_loss = fm_forward_time_cond(model, x0, img, xt, rand_t)
            optimizer.zero_grad()
            img_loss.backward()
            optimizer.step()

            training_losses_time.append(img_loss)

        model.eval()
        testing_results.append([fm_sample_time(model, (28, 28), 299) for _ in range(5)])

        lr_scheduler.step()
    return training_losses_time, testing_results
```
The training curve is as shown below:

![image](/post_assets/280_hw2_result/time_cond/result_A_losses.png)

### Part 2.3 Sampling from the UNet
The sampling algorithm is as defined above (`fm_sample_time`).
The results are as follows:

![image](/post_assets/280_hw2_result/time_cond/result_B_samples.png)

### Part 2.4 Adding Class-Conditioning to UNet
This is once again an edit to be made on the original UNet class:
```
class ClassConditionalUNet(UnconditionalUNet):
    def __init__(
        self,
        in_channels: int,
        num_classes: int,
        num_hiddens: int,
    ):
        super().__init__(in_channels=in_channels, num_hiddens=num_hiddens)
        self.fc_time_b3 = FCBlock(1, 2 * num_hiddens)
        self.fc_time_b2 = FCBlock(1, num_hiddens)
        self.fc_class_b3 = FCBlock(num_classes, 2 * num_hiddens)
        self.fc_class_b2 = FCBlock(num_classes, num_hiddens)

    def forward(
        self,
        x: torch.Tensor,
        c: torch.Tensor,
        t: torch.Tensor,
    ) -> torch.Tensor:
        """
        Args:
            x: (N, C, H, W) input tensor.
            t: (N,) normalized time tensor.

        Returns:
            (N, C, H, W) output tensor.
        """
        assert x.shape[-2:] == (28, 28), "Expect input shape to be (28, 28)."
        a0_output = self.start_conv_block(x)
        a1_output = self.block_a1(a0_output)
        a2_output = self.block_a2(a1_output)
        a3_output = self.block_a3(a2_output)

        b3_output = (
            self.fc_class_b3(c)[..., None, None] * self.block_b3(a3_output)
            + self.fc_time_b3(t)[..., None, None]
        )  # change here
        b3_cat = torch.cat((a2_output, b3_output), dim=1)

        b2_output = (
            self.fc_class_b2(c)[..., None, None] * self.block_b2(b3_cat)
            + self.fc_time_b2(t)[..., None, None]
        )  # change here
        b2_cat = torch.cat((a1_output, b2_output), dim=1)

        b1_output = self.block_b1(b2_cat)
        b0_output = torch.cat((a0_output, b1_output), dim=1)
        end_conv_block_output = self.end_conv_block(b0_output)
        end_conv_output = self.end_conv_layer(end_conv_block_output)
        return end_conv_output
```
We first construct the following utilizations to implement the loss computation and sampling procedure for our class-conditioned UNet.
```
def fm_forward_class(
    unet: ClassConditionalUNet,
    x_0: torch.Tensor,
    x_1: torch.Tensor,
    x_t: torch.Tensor,
    ts: torch.Tensor,
    c: torch.Tensor,
    p_uncond: float,
) -> torch.Tensor:
    unet.train()
    c_onehot = torch.nn.functional.one_hot(c, num_classes=10)
    c_onehot = c_onehot * (torch.rand(c.shape) >= p_uncond)[..., None]
    input_c = c_onehot.float().to(device)
    true_velocity = x_1 - x_0
    true_velocity = true_velocity.to(device)
    pred_velocity = unet(x_t.to(device), input_c.to(device), ts[..., None].to(device))
    batch_loss = torch.nn.functional.mse_loss(true_velocity, pred_velocity)

    return batch_loss.mean()


@torch.inference_mode()
def fm_sample_class(
    unet: ClassConditionalUNet,
    c: torch.Tensor,
    img_wh: tuple[int, int],
    num_ts: int,
    guidance_scale: float = 5.0,
) -> torch.Tensor:
    unet.eval()
    # YOUR CODE HERE.
    timesteps = torch.linspace(0, 1, num_ts).to(device)[..., None]
    curr_step_img = torch.randn((1, 1, *img_wh)).to(device)
    c_onehot = torch.nn.functional.one_hot(c, num_classes=10)
    c_onehot_zero = torch.zeros_like(c_onehot)

    c_onehot = c_onehot.float().to(device)
    c_onehot_zero = c_onehot_zero.float().to(device)

    for curr_t in timesteps:

        eps_u = unet(curr_step_img, c_onehot_zero, curr_t)
        eps_c = unet(curr_step_img, c_onehot, curr_t)
        eps = eps_u + guidance_scale * (eps_c - eps_u)

        curr_step_img += eps / num_ts
    return curr_step_img
```
The trainer is as formulated below:
```
def fm_class_cond_train():
    # Define based on assignment:
    num_hiddens = 64
    in_channels = 1

    # Train with fm_forward, borrow code from above
    model = ClassConditionalUNet(
        in_channels=in_channels, num_classes=10, num_hiddens=num_hiddens
    )
    model = model.to(device)

    train_loader = DataLoader(dataset, batch_size=64, shuffle=True)
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)
    n_epoch = _NUM_EPOCHS
    lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(
        optimizer, gamma=0.1 ** (1 / n_epoch)
    )

    training_losses_class = []
    model.eval()

    for _ in range(n_epoch):
        model.train()
        running_loss = 0
        for img, lbl in tqdm.tqdm(train_loader):
            rand_t = torch.rand(size=(img.shape[0],))
            x0 = torch.randn(img.shape)
            rand_t_reshaped = rand_t.reshape((rand_t.shape[0], 1, 1, 1))
            xt = (1 - rand_t_reshaped) * x0 + rand_t_reshaped * img
            img_loss = fm_forward_class(model, x0, img, xt, rand_t, lbl, 0.1)
            optimizer.zero_grad()
            img_loss.backward()
            optimizer.step()

            training_losses_class.append(img_loss)
        lr_scheduler.step()
    return training_losses_class, model
```

The training loss curve is as illustrated below:

![image](/post_assets/280_hw2_result/class_cond/training_loss_curve.png)

### Part 2.5 Sampling from the Class-Conditioned UNet
The desired sampled images are as follows.

At the 5th epoch of training:
![image](/post_assets/280_hw2_result/class_cond/epoch_5_sampled_images.png)

At the 10th epoch of training:
![image](/post_assets/280_hw2_result/class_cond/epoch_10_sampled_images.png)

At the 20th epoch of training:
![image](/post_assets/280_hw2_result/class_cond/sampled_images.png)

You can notice that it is roughly equivalently effective across the epochs.